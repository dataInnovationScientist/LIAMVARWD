{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns; sns.set()\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "from collections import Counter\n",
    "from scipy.spatial import distance\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import supporting_functions as sf\n",
    "import hypercubes\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layered Integration Approach for Multi-view Analysis of Real-world Datasets\n",
    "In this notebook we validate the *Layered Integration Approach for Multi-view Analysis of Real-world Datasets*.\n",
    "\n",
    "The proposed layered integration methodology is demonstrated on SCADA data from the wind turbine dataset from [Engie](https://opendata-renewables.engie.com/explore/index). In this dataset 8 years of data from 4 wind turbines is available. This is an excellent example of a multi-source (multiple wind turbines) and multi-view (multiple different in nature subsets of parameters) dataset, which is exactly where the layered integration approach is designed for.\n",
    "\n",
    "\n",
    "The initial aim of the studied experimental scenario is to identify and **characterise potentially different operating modes across\n",
    "the fleet**. Notice that wind turbines can have several different operating modes, e.g. working at full speed, reduced speed in order to limit the noise burden on the surroundings, tailored production due to oversupply on the net and others. Subsequently, the ultimate goals is to **derive distinctive profiles of production performance** and establish an explicit link between those performance profiles and the characterised operating modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Overview of the Engie Dataset\n",
    "The dataset should be first downloaded from [here](https://opendata-renewables.engie.com/explore/index), and the path should be changed.\n",
    "\n",
    "As one can in the plots below, the 4th wind turbine has a big loss of values between 2013 and 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR='../../../Hymop_Engie/repository/hymop-Engie/data/'\n",
    "\n",
    "df_engie = pd.read_csv(DATA_DIR + 'extended_clean_sensor_data.csv', parse_dates=[0])\n",
    "\n",
    "print(\"Amount of entries in database: \\t\"+ str(df_engie.shape[0]))\n",
    "print(\"Amount of wind turbines: \\t\"+str(len(df_engie.Wind_turbine_name.unique())))\n",
    "print(\"Amount of features: \\t\\t\"+ str(df_engie.shape[1]))\n",
    "\n",
    "print(\"\\nstart: \\t\"+str(min(df_engie.Date_time)))\n",
    "print(\"end: \\t\"+str(max(df_engie.Date_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df_engie.reset_index(), row=\"Wind_turbine_name\", height=1.7, aspect=8);\n",
    "g.map(sns.lineplot, \"Date_time\", \"P_avg\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Data Preprocessing\n",
    "## 2.1 Eliminating Correlated Parameters\n",
    "A difficulty in this dataset is that some features are not clearly defined, which makes it difficult to see which parameters are redundant. Too get more insights on whether a feature is derived from another, we check the correlation between all features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20,20))\n",
    "sns.heatmap(df_engie.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this correlation matrix, some variables with a very high correlation are observed. We do not want this because this will cause an unbalanced focus in the clustering approach. \n",
    "\n",
    "Below we enumerate the values with the highest correlations and explain which ones are removed:\n",
    "* DCs_avg (Generator converter speed), Ds_avg (Generator speed) and Rs_avg (Rotor speed)\n",
    "    * The Generator converter speed and generator speed are almost exactly as shown in the table below.\n",
    "    * Despite the lower values of the Rotor speed in rpm, it seems to be very correlated to the other two mentioned features. This might be explained that they are connected to each-other, but have a different speed thanks to the gear connection. The difference is an almost constant factor around 100.\n",
    "    * The Gearbox bearing temperatures show also a high correlation to those 3 features. Which is expected due to the fact that increasing speed is a cause of increasing temperature of the bearing.\n",
    "    * For this reason, we will **not use the generator converter speed and rotor speed** from now on.\n",
    "* P_avg (Active power), S_avg (Apparent power), Cm_avg (Converter torque) and Rm_avg (Torque)\n",
    "    * The power and active power are almost equal.\n",
    "    * The torque is the force which is finally transformed into active power, which explains the big correlation.\n",
    "    * Since we have here features of the two views, we will only **remove the apparent power and the converter torque**.\n",
    "* Ws1_avg, Ws2_avg and Ws_avg\n",
    "    * As expected do they also have a big correlation.\n",
    "    * We will **remove Ws1_avg and Ws2_avg**.\n",
    "* Ya_avg (Nacelle angle) and Na_c_avg (corrected nacelle angle)\n",
    "    * Evidently, we'll only use the corrected nacelle angle.\n",
    "\n",
    "*Remark: After [normalization](#2.2-Normalize), one could also observe a very similar distribution between the highly correlated variables.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Identification of the different in nature subset of parameters\n",
    "Intrinsically, the wind turbine data contains three subsets of parameters:\n",
    "\n",
    "- **Operational View**: sensor measurements about internal working of the turbine\n",
    "    - Ba_avg:  Pitch angle\n",
    "    - Db1t_avg: Generator bearing 1 temperature (bearing = lager)\n",
    "    - Db2t_avg: Generator bearing 2 temperature\n",
    "    - Dst_avg:  Generator stator temperature\n",
    "    - Rbt_avg:  Rotor bearing temperature\n",
    "    - Gb1t_avg: Gearbox bearing 1 temperature\n",
    "    - Gb2t_avg: Gearbox bearing 2 temperature\n",
    "    - Git_avg:  Gearbox inlet temperature\n",
    "    - Gost_avg: Gearbox oil sump temperature\n",
    "    - Yt_avg:   Nacelle temperature\n",
    "    - (Ya_avg:   Nacelle angle)\n",
    "    - Na_c_avg: Nacelle angle corrected\n",
    "    - (Rs_avg:   Rotor speed)\n",
    "    - Rm_avg:   Torque\n",
    "    - (DCs_avg: Generator converter speed)\n",
    "    - Ds_avg:  Generator speed\n",
    "    - (Cm_avg:  Converter torque)\n",
    "    \n",
    "- **Contextual View**: active power as a function of exogenous factors\n",
    "\n",
    "    - (Ws1_avg:  First anemometer on the nacelle (wind speed))\n",
    "    - (Ws2_avg:  Second anemometer on the nacelle (wind speed))\n",
    "    - Ws_avg:   Average wind speed\n",
    "    - Wa_avg:   Absolute wind direction\n",
    "    - Wa_c_avg: Absolute wind direction corrected\n",
    "    - Ot_avg:   Outdoor temperature\n",
    "    - Rt_avg:   Hub temperature (the hub height is the distance from the ground to the center-line of the turbine rotor)\n",
    "    - Nf_avg:   Grid frequency \n",
    "\n",
    "- **Output:**\n",
    "    - P_avg:    Active power\n",
    "    - Q_avg:   Reactive power\n",
    "    - (S_avg:   Apparent power (Should be $ \\sqrt{P^2 + Q^2}$ ) )\n",
    "    - Cosphi_avg: Power factor (Should be $ \\frac{P}{S}$ )\n",
    "    \n",
    "The features between brackets are left away since they are very correlated to other features in the same view.\n",
    "\n",
    "Later on, the parameters of the operational view will be used in the 1st layer (individual analysis). In the 2nd layer (mediation analysis) we will use the parameters of the contextual view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "operational_features = ['Ba_avg', 'Db1t_avg', 'Db2t_avg', 'Dst_avg', 'Rbt_avg', 'Gb1t_avg', 'Gb2t_avg', \n",
    "                        'Git_avg', 'Gost_avg', 'Na_c_avg', 'Rm_avg', 'Ds_avg'] #, 'DCs_avg', 'Rs_avg', 'Cm_avg', 'Ya_avg'\n",
    "\n",
    "contextual_features = ['P_avg', 'Q_avg', 'Cosphi_avg', 'Ws_avg', \n",
    "                       'Wa_avg', 'Wa_c_avg', 'Ot_avg', 'Yt_avg', 'Rt_avg', 'Nf_avg'] #, 'S_avg', 'Ws1_avg', 'Ws2_avg'\n",
    "\n",
    "features_dict = {'Ba_avg':  'Pitch angle', 'Db1t_avg': 'Generator bearing 1 temperature', 'Db2t_avg': 'Generator bearing 2 temperature', 'Dst_avg':  'Generator stator temperature', \n",
    "                 'Rbt_avg':  'Rotor bearing temperature', 'Gb1t_avg': 'Gearbox bearing 1 temperature', 'Gb2t_avg': 'Gearbox bearing 2 temperature', 'Git_avg':  'Gearbox inlet temperature', \n",
    "                 'Gost_avg': 'Gearbox oil sump temperature', 'Yt_avg':   'Nacelle temperature', 'Ya_avg':   'Nacelle angle', 'Na_c_avg': 'Nacelle angle corrected', 'Rs_avg':   'Rotor speed', \n",
    "                 'Rm_avg':   'Torque', 'DCs_avg': 'Generator converter speed', 'Ds_avg':  'Generator speed', 'Cm_avg':  'Converter torque', 'P_avg':    'Active power', \n",
    "                 'Q_avg':   'Reactive power', 'S_avg':   'Apparent power', 'Cosphi_avg': 'Power factor', 'Ws1_avg':  'First anemometer on the nacelle', \n",
    "                 'Ws2_avg':  'Second anemometer on the nacelle', 'Ws_avg':   'Average wind speed', 'Wa_avg':   'Absolute wind direction', 'Wa_c_avg': 'Absolute wind direction corrected', \n",
    "                 'Ot_avg':   'Outdoor temperature', 'Rt_avg':   'Hub temperature' , 'Nf_avg':   'Grid frequency' }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Separate the dataset\n",
    "\n",
    "To make the code in the following steps more easy, we create a dictionary which contains a dataframe for each wind turbine separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turbines = df_engie['Wind_turbine_name'].unique()\n",
    "\n",
    "dict_turbines_uncleaned = {}\n",
    "for i, turbine in enumerate(turbines):\n",
    "    dict_turbines_uncleaned[turbine] = df_engie[df_engie['Wind_turbine_name']==turbine].copy().sort_index()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Noise removal\n",
    "\n",
    "In this cell we remove the timestamps which we consider as noise. This is illustrated on the image below.\n",
    "\n",
    "There are 4 methods we use to filter points:\n",
    "- We remove points which have a negative active power or negative wind speed\n",
    "- We remove points which have an uncommon wind speed-active power relation (by sparse cubes)\n",
    "- We remove points which deviate to much from the expected active power on the **positive side** (hypercube approach)\n",
    "- We remove points which deviate to much from the expected active power on the **negative side** (hypercube approach)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=1)\n",
    "\n",
    "dict_turbines = sf.remove_noise(dict_turbines_uncleaned, hypercubes, \n",
    "                                      remove_negative=True, min_cube_count=1, max_pos_deviation=500, max_neg_deviation=-1250)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Normalize the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to give equal priority to each parameter, they need to be normalized.\n",
    "The normalization is applied separately on each parameter and wind turbine by calculating the min-max score for each of the parameters.\n",
    "\n",
    "The histograms of the min-max score results of each parameter from the first wind turbine are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale=0.8)\n",
    "\n",
    "# Select parameters to normalize\n",
    "parameters = list(df_engie.columns)\n",
    "parameters.remove('Date_time')\n",
    "parameters.remove('Wind_turbine_name')\n",
    "\n",
    "# Make dictionary to separate the NORMALIZED data from each wind turbine:\n",
    "dict_turbines_norm = {}\n",
    "\n",
    "for i, turbine in enumerate(turbines):\n",
    "    df_one_turbine = dict_turbines[turbine].copy()\n",
    "    \n",
    "    for p in parameters:\n",
    "        df_one_turbine[p] = (df_one_turbine[p] - df_one_turbine[p].min())/(df_one_turbine[p].max() - df_one_turbine[p].min()) #min-max\n",
    "        \n",
    "    dict_turbines_norm[turbine] = df_one_turbine\n",
    "    \n",
    "# Plot the histograms of each feature from the First wind turbine\n",
    "hist = dict_turbines_norm[turbines[0]].hist(figsize=(20,10), bins=20)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. LAYER 1: Individual analysis\n",
    "In the proposed layered integration approach, we start with the individual analysis. In here we will cluster the timestamps of each of the 4 wind turbines separately, based on the subset of **operating parameters**. As shown in [Section 2.2](#2.2-Identification-of-the-different-in-nature-subset-of-parameters), this are 12 parameters. This will result in turbine dependent operating modes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Determine the appropriate amount of clusters per dataset\n",
    "We use the k-means approach to do each clustering. The initial centers are selected by the smart 'k-means++' approach. Since the k-means++ algorithm still uses a random initial start, we can (and do) repeat it 5 times. Only the best result is used to plot.\n",
    "\n",
    "For each of the wind turbines, the amount of clusters is determined separately by multiple validation techniques: \n",
    "- **elbow method**: In here we plot the sum of the squared distance between each point and the centroid of its cluster. If there is a sudden change in decrease, the optimal amount of clusters might be there\n",
    "- **connectivity**: In here we look to the 10 closest neighbours of each point and we add a penalty if they are not in the same cluster. In here we again search for a knot.\n",
    "- **silhouette**: The higher, the better. Since the silhouette algorithm is very slow, it is calculated based on only 5% of the data. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually.\n",
    "- **Calinski Harabasz**: the higher the better\n",
    "- **Davies Bouldin**: the lower the better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ATTENION: this cell takes around 50 minutes to run!\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "for i, turbine in enumerate(turbines):\n",
    "    print(\"Calculation of wind turbine \" + str(i+1) + \"...\")\n",
    "    df_One_norm = dict_turbines_norm[turbine].reset_index()\n",
    "    \n",
    "    sf.plot_kmeans_validation(df_One_norm.loc[:, operational_features], sample_size=0.1)\n",
    "\n",
    "print(\"\\t Elapsed time: \"+ str( np.round((timeit.default_timer() - start_time)/60, 2)) + \" minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deciding the proper amount of clusters is a very hard thing to do. We use the majority voting technique based on the results plotted above to decide the optimal amount of clusters per wind turbine. This results in an optimal of 3, 4, 4 and 3 clusters, in the order of the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Compare power curve & torque curve of the obtained operating modes\n",
    "Based on the most optimal amount of clusters for each dataset, we do the final clusters of the timestamps. Below we plot the power curve, torque curve and amount of points per cluster. Since it is both interesting to see the clusters on a separate plot as in the same plot, both of them are plotted below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = [3, 4, 4, 3]\n",
    "\n",
    "# Define colors (which will be consitently used during this notebook)\n",
    "colorpal = sns.color_palette(\"Set3\", np.sum(n_clusters))\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "dict_turbines_norm = sf.do_individual_clustering(turbines, dict_turbines_norm,n_clusters, operational_features, dict_turbines, colorpal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.visualize_individual_clustering_per_clusters(colorpal, n_clusters, dict_turbines, turbines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Compare the difference of the parameters in the operating modes\n",
    "\n",
    "In the cell below we plot for **one turbine** the distributions of each parameter for the clusters separately. This indicates what the main differences are between the operating modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select one turbine:\n",
    "turbine_nr = 0 #Can be 0, 1, 2 or 3\n",
    "turbine = turbines[turbine_nr]\n",
    "df_One = dict_turbines[turbine]\n",
    "\n",
    "color_index = sum(n_clusters[0:turbine_nr])\n",
    "colors = colorpal[color_index:color_index+n_clusters[turbine_nr]]\n",
    "\n",
    "for f, feature in enumerate(operational_features):\n",
    "    g1 = sns.FacetGrid(df_One, col=\"Label\", height=3, aspect = 1.5, hue='Label', palette=colors)\n",
    "    g1.map(plt.hist, feature, linewidth=0, alpha=1, density=True, bins=20)\n",
    "    g1.set_axis_labels(features_dict[feature], \"Density\")\n",
    "    g1.set_titles(\"Turbine \" + turbine + \", Cluster {col_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Operating mode 1 differs mainly from the others in:\n",
    "    - Generator stator temperature\n",
    "    - Gearbox bearing 1 temperature\n",
    "    - Gearbox bearing 2 temperature\n",
    "    - Gearbox inlet temperature\n",
    "    - Torque\n",
    "- All operating modes differ clearly from the others in:\n",
    "    - Nacelle angle corrected\n",
    "    - Generator speed\n",
    "\n",
    "\n",
    "## 3.4 Investigate the minimum and maximum value of each parameter in each cluster\n",
    "Each cluster will define a range of allowable values for each operational parameter and thus generates parametric characterisation of the operating mode. In this way, the pool of clusters produced for the fleet leads to the construction of a repository of operating modes, each one characterized by two vectors of low and high values (see Table below).\n",
    "For each operating mode, the high and low ranges for each parameter in the operating mode can also be considered as allowable ranges for the corresponding parameter, provided the other parameters are within the other ranges specified for this mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = sf.create_table_endogenous_view(turbines, dict_turbines, operational_features)\n",
    "table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LAYER 2: Mediation Analysis \n",
    "\n",
    "In this layer, we pursue a way to derive an alternative representation of each operating mode in terms of expected performance. The richness of our multivariate data allows to consider an alternative view for each cluster of timestamps generated in the previous layer.\n",
    "\n",
    "To do this, we first divide the solution space of each operating mode into hypercubes with the dimensions wind speed, wind direction and ambient temperature. Next, we calculate for each of the hypercubes the kernel density estimation (KDE) of the active power. And than finally, we obtain a profile for each operating mode by summing the weighted KDEs together and normalize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Divide the space into hypercubes\n",
    "\n",
    "We divide the timestamps in hypercubes based on wind speed, wind direction and ambient temperature. These are all [exogenous parameters](#2.2-Identification-of-the-different-in-nature-subset-of-parameters). Each hypercube has a size of 2 m/s (wind speeds), 72 degrees (wind direction) and 10 degrees (outside temperature). However, the method seems to be very robust to the hypercube size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an empty Dataframe \n",
    "cluster_features = pd.DataFrame()\n",
    "\n",
    "# Define the amount of splits per feature (for binning)\n",
    "dict_features = {'Wa_c_avg': 6, 'Ws_avg':11, 'Ot_avg':6} \n",
    "ref_vars = list(dict_features.keys())\n",
    "target_var = ['P_avg']\n",
    "dat_types = ['angular','numerical','numerical']\n",
    "\n",
    "# Define cubes for all \n",
    "_, cube_specs, _ = hypercubes.define_binning(dict_turbines[turbines[0]], ref_vars, dict_features)\n",
    "\n",
    "# Give each hypercube an index number\n",
    "cube_ids = sf.make_cube_ids(cube_specs, ref_vars)\n",
    "\n",
    "for turbine in turbines:\n",
    "    # Enricht dataframe with hypercube numbers\n",
    "    dict_turbines[turbine],_ = hypercubes.define_cubes_in_df(dict_turbines[turbine].copy(), ref_vars, target_var[0], drop_stats=True, reset_idx=True,\n",
    "                                       cube_specs=cube_specs, cube_ids=cube_ids, assign_nearest_cube=False,\n",
    "                                       dat_types=dat_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now remove all hypercubes cubes which contain less then 10 points. Since these hypercubes are so sparse, the features we will derive later on within those hypercubes would not be reliable.\n",
    "\n",
    "In the plot below one can see the effect of this filtering. Each operating mode got rid of around 30% of the hypercubes, while almost all points are represented by their hypercube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_POINTS_IN_CUBE = 10\n",
    "df_cube_counts, cluster_labels = sf.visualize_sparse_hypercubes(MIN_POINTS_IN_CUBE, turbines, dict_turbines, n_clusters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Calculate the kernel density estimations\n",
    "Below, we estimate the density of the active power within each hypercube. This is estimated by the KDE approach where we use a Gaussian kernel and the Silverman rule of thumb to calculate the bandwidth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KDEs, points_per_cube = sf.calculate_KDE_per_hypercube(turbines, n_clusters, dict_turbines, MIN_POINTS_IN_CUBE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Combine densities per cluster as an endogenous profile for each internal operating mode\n",
    "Profiling is done as following: We sum all the KDEs of the same wind turbine (weighted by the amount of points which are present in the hypercubes), and normalized by the total amount of (non removed) points. \n",
    "\n",
    "Below, one can see all original KDEs on the left (per operating mode), and the constructed performance profile (mixture probability distribution) on the right. One already can observe different patterns between operating modes of the same wind turbine, and similar patterns between operating modes of different wind turbines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_densities = sf.construct_engdogenous_profiles(turbines, n_clusters, points_per_cube, KDEs, colorpal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Calculate the pairwise distance\n",
    "Below, one can see a heatmap of the distance matrix between the different mixture probability distributions. Distance is here defined as the Euclidean distance between the points of the mixture probability distribution with an active power of [0, 2.5, 5 ... 2500]. Once more, one can see some operating modes from different wind turbines with very low correlations, meaning they might have a similar behavior (based on the exogenous parameters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance matrix\n",
    "cluster_distances = pairwise_distances(sum_densities, metric='euclidean')\n",
    "\n",
    "# Visualise heatmap\n",
    "plt.figure(figsize = (15,12))\n",
    "OM_labels = ['OM '+str(i) for i in list(range(1, sum(n_clusters)+1))]\n",
    "sns.heatmap(cluster_distances, annot=True, linewidths=1, square=True, xticklabels=OM_labels, yticklabels=OM_labels)\n",
    "plt.yticks(rotation=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LAYER 3: Integration Analysis \n",
    "\n",
    "In this layer, we aim to combine the operational view (from [Layer 1](#3.-LAYER-1:-Individual-analysis)) with the exogenous view (from [Layer 2](#4.-LAYER-2:-Mediation-Analysis)). To do this, the obtained performance profiles (mixture distributions) per individual operating mode are pooled together and subjected to k-means clustering. This way we will obtain fleet-wide performance profiles, which we can trace back to the allowable ranges of turbine specific endogenous parameters (see [the table](#3.4-Investigate-the-minimum-and-maximum-value-of-each-parameter-in-each-cluster) which we constructed before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Determine the appropriate amount of clusters\n",
    "\n",
    "In here the optimal number of **fleet-wide** operating modes is investigated. We use again the Euclidean distance as distance measure. However, other measures as the Jensen Shannon distance might be used in future.\n",
    "\n",
    "Following the majority voting system, the optimal amount of clusters (fleet-wide operating modes) is 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.plot_kmeans_validation(sum_densities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Cluster the operating modes into fleet-wide operating modes, based on their individual performance profile\n",
    "We do this again with k-means clustering. From the previous cell we know that k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_reduced_clusters = 3\n",
    "\n",
    "# Do clustering:\n",
    "kmeans = KMeans(n_reduced_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "final_labels = kmeans.fit_predict(sum_densities) # Only use the operational features in the MAP step\n",
    "\n",
    "# Map fleet-wide labels to letters:\n",
    "OM_dict = dict({0:'A', 1:'B', 2:'C', 3:'C', 4:'C', 5:'D'})\n",
    "final_labels = [OM_dict[label] for label in final_labels]\n",
    "\n",
    "dictionary = dict(zip(df_cube_counts['cluster'], final_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show the power curve and torque curve for each of the fleet-wide operating modes\n",
    "As we can see on the plots below, each operating mode are clearly different in the two curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make one big dataframe with the fleet-wide operating mode labels in it\n",
    "cluster_centers_reduced = pd.concat(dict_turbines, ignore_index=True)\n",
    "cluster_centers_reduced[\"Operating_mode\"] = cluster_centers_reduced.Wind_turbine_name + '_' + cluster_centers_reduced.Label.astype(str)\n",
    "cluster_centers_reduced[\"FW_Operating_mode\"] = cluster_centers_reduced[\"Operating_mode\"].replace(dictionary)\n",
    "\n",
    "# Plot the points within each cluster\n",
    "sf.plot_power_and_torque(cluster_centers_reduced, colorpal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the points of the fleet-wide operating modes PER TURBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, dataset in dict_turbines.items():\n",
    "    \n",
    "    dataset[\"Operating_mode\"] = dataset.Wind_turbine_name + '_' + dataset.Label.astype(str)\n",
    "    dataset[\"FW_Operating_mode\"] = dataset[\"Operating_mode\"].replace(dictionary)\n",
    "    \n",
    "    # 1. Plot power curve\n",
    "    g1 = sns.FacetGrid(dataset, col=\"FW_Operating_mode\", height=3, aspect = 1.5, col_order=['A','B','C'])\n",
    "    g1.map(plt.scatter, \"Ws_avg\", \"P_avg\", s=1,linewidth=0, alpha = 1)\n",
    "    g1.set(xlim=(0, 20), ylim=(-100, 2100))\n",
    "    g1.set_axis_labels( \"Wind speed [m/s]\", \"Active power [kW]\")\n",
    "    g1.set_titles(\"Operating mode {col_name}\")\n",
    "    g1.fig.suptitle('Power curves of turbine '+name, y=1.02)\n",
    "    g1.add_legend()\n",
    "    for lh in g1._legend.legendHandles: # This makes the legend visible\n",
    "        lh.set_alpha(1)\n",
    "        lh._sizes = [50]\n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Calculate **fleet-wide** performance profiles of each fleet-wide operating mode\n",
    "\n",
    "Now we calculate the **fleet-wide** performance profiles by summing the individual probabilistic distributions of each operating mode. The mixture weights have been computed as the number of points in the corresponding cluster from layer 1, normalized by the total number of points in the given fleet-wide cluster. The resulting very distinctive fleet-wide performance profiles denoted as A,\n",
    "B and C are depicted below.\n",
    "\n",
    "*Remark: This is very similar as in the [previous layer](#4.3-Combine-densities-per-cluster-as-an-endogenous-profile-for-each-internal-operating-mode), but one level higher.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf.construct_fleetwide_performance_profiles(n_reduced_clusters, final_labels, sum_densities, df_cube_counts, points_per_cube, colorpal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now each of the fleet-wide performance profiles can be traced back to a subset of individual operating modes ([this Table](#3.4-Investigate-the-minimum-and-maximum-value-of-each-parameter-in-each-cluster)), resulting in fleet-wide (composite) operating modes, which we also denote with A, B and C: A={1,3,5,6,8,9,12,13}; B={2,4,10,14}; C ={7,11}. It is interesting to observe that the composite operating mode linked to profile C can be traced back to only two of the four wind turbines. The derived fleet-wide (composite) operating modes, each associated with a very distinctive performance profile (see Figure above), can now be used to label the fleet data as follows: \n",
    "\n",
    "1. For each timestamp, consider the values of the 12 operational parameters.\n",
    "2. Determine to which operating mode they can be assigned ([see this Table](#3.4-Investigate-the-minimum-and-maximum-value-of-each-parameter-in-each-cluster)). This is different per wind turbine.\n",
    "3. Identify the fleet-wide (composite) operating mode to which the identified mode belongs.\n",
    "4. Subsequently, assign the corresponding letter A, B, C or D (not seen) to the timestamp. \n",
    "\n",
    "In this way, each dataset per turbine can be converted into A, B, C or D code (as a DNA sequence), which can be very insightful for monitoring purposes (e.g. long periods of B would signify optimal performance), but is also a powerful representation enabling\n",
    "more advanced applications, e.g.: \n",
    "- mining the fleet data for interesting patterns such as transitions between operating modes\n",
    "- zooming in periods with too many Ds\n",
    "- training a predictor of expected production on historical data to be used to detect deviations during real-time operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusion and future work\n",
    "\n",
    "We have proposed a novel data analysis approach that can be used for multi-view analysis and integration of heterogeneous real-world datasets originating from multiple sources. The validity and the potential of the proposed approach has been demonstrated on a real-world dataset of a fleet of wind turbines. The obtained results are very encouraging. The method is very efficient and robust in detecting characteristic operating modes across the fleet. Subsequently, distinctive performance profiles are derived and associated with each operating mode, which enable converting the fleet data into powerful letter code suitable for more advanced mining.\n",
    "\n",
    "\n",
    "For future work, we are interested to extend our research in the following directions: \n",
    "1. Fine tune further the method by using e.g. an adaptive hypercube binning.\n",
    "2. Testing different experimental scenarios e.g. comparing different time periods from the same wind turbine. \n",
    "3. Consider additional validation use cases dealing with multi-source datasets e.g. mobility or manufacturing data. \n",
    "4. Extend further the method by exploiting the possibility to covert the fleet data into letter code.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
